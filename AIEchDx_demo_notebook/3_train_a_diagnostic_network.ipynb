{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "frames = 45\n",
    "echo_cato = [\"ASD\",\"DCM\",\"HP\",\"MI\",\"NORM\"]\n",
    "number = 1000 # one sample\n",
    "\n",
    "\n",
    "dir_ = \"....\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "def generate_npy(txt_dir, data_dir, frames, clips_no=5):\n",
    "    \"\"\"\n",
    "    channel last as keras\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # read txt file\n",
    "    txt = pd.read_csv(txt_dir,header = None)\n",
    "    txt = txt.rename(index=str, columns={0: \"photo_name\"})\n",
    "    txt[\"dir_name\"] = \"\"\n",
    "    txt[\"rank\"] = \"\"\n",
    "    \n",
    "    # k = 0\n",
    "    for i, name in enumerate(txt[\"photo_name\"]):    \n",
    "        name_list = name[:-4].split(\"_\")\n",
    "        txt[\"rank\"][i] = float(name_list[-1])\n",
    "        length = 4 + len(name_list[-1]) + 1\n",
    "        txt[\"dir_name\"][i] = name[:-length]\n",
    "        #k = k + 1\n",
    "        #if k>50:\n",
    "        #    break;\n",
    "    \n",
    "    txt = pd.DataFrame(txt)\n",
    "    \n",
    "    # read data file\n",
    "    \n",
    "    data = pd.read_csv(data_dir,header = None)\n",
    "    feat_cols = [ 'pixel_'+str(i) for i in range(data.shape[1]) ]\n",
    "    data = pd.DataFrame(data.values,columns=feat_cols)\n",
    "    \n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    \n",
    "    # merge data and txt togather\n",
    "    \n",
    "    data2 = data.copy()\n",
    "    data2[\"photo_name\"] = \"\"\n",
    "    data2[\"dir_name\"] = \"\"\n",
    "    data2[\"rank\"] = \"0\"\n",
    "    \n",
    "    for i, value in enumerate(txt.iloc[:,0]):\n",
    "        data2.loc[i,\"photo_name\"] = value\n",
    "    for i, value in enumerate(txt.iloc[:,1]):\n",
    "        data2.loc[i,\"dir_name\"] = value\n",
    "    for i, value in enumerate(txt.iloc[:,2]):\n",
    "        data2.loc[i,\"rank\"] = value  \n",
    "    \n",
    "    # sort data2 into the right order\n",
    "    \n",
    "    data2 = data2.sort_values(by=['dir_name','rank'])\n",
    "    data2 = data2.reset_index(drop=True)\n",
    "    \n",
    "    # transfer 2048 dimension data to 512*15 data\n",
    "    \n",
    "    sample_name = data2.dir_name.unique()\n",
    "    length = len(sample_name)\n",
    "    n_array = np.zeros(shape=(length*clips_no, frames, data.shape[1]))\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    for name in data2.dir_name.unique():\n",
    "        (x,y) = data2[data2[\"dir_name\"]== name].shape\n",
    "        npy = data2[data2[\"dir_name\"]== name].iloc[:,:data.shape[1]].values\n",
    "        if x<frames:\n",
    "            kk = 0\n",
    "            for cn in range(clips_no):\n",
    "                for i in range(frames):\n",
    "                    n_array[count,i,::] = npy[i%x]\n",
    "                count = count + 1\n",
    "        elif (x>=frames)&(x<(frames+clips_no-1)):\n",
    "            kk = 1\n",
    "            for cn in range(clips_no):\n",
    "                t = cn%(x-frames+1)\n",
    "                for i in range(frames):\n",
    "                    tt = t+i\n",
    "                    n_array[count,i,::] = npy[tt]\n",
    "                count = count + 1\n",
    "        elif x>=(frames+clips_no-1):\n",
    "            kk = 2\n",
    "            for cn in range(clips_no):\n",
    "                clips_2 = float(clips_no - 1)\n",
    "                t = int((x-frames)/clips_2*cn)\n",
    "                for i in range(frames):\n",
    "                    tt = t+i\n",
    "                    n_array[count,i,::] = npy[tt]\n",
    "                count = count + 1\n",
    "    print(count, length)\n",
    "    return n_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. generate training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = {}\n",
    "for index, name in enumerate(echo_cato):\n",
    "    \n",
    "    print(name)\n",
    "    txt_dir = dir_ + \"train_\" + name + \"_name.txt\"\n",
    "    data_dir = dir_ + \"train_\" + name + \"_data.txt\"\n",
    "    frames = frames\n",
    "    # clips_no=5\n",
    "    array = generate_npy(txt_dir, data_dir, frames, clips_no=5)\n",
    "    \n",
    "    if array.shape[0]<400:\n",
    "        array = np.concatenate((array, array), axis=0)\n",
    "    \n",
    "    record[name] = array.shape\n",
    "    y_array = np.ones(shape=(array.shape[0],1))\n",
    "    y_array = y_array*(index)\n",
    "    \n",
    "    if index == 0:\n",
    "        X_train = array.copy()\n",
    "        y_train = y_array.copy()\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train, array), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_array), axis=0)\n",
    "\n",
    "\n",
    "record = {}\n",
    "for index, name in enumerate(echo_cato):\n",
    "    \n",
    "    print(name)\n",
    "    txt_dir = dir_ + \"validation\" + name + \"_name.txt\"\n",
    "    data_dir = dir_ + \"validation\" + name + \"_data.txt\"\n",
    "    frames = frames\n",
    "    # clips_no=5\n",
    "    array = generate_npy(txt_dir, data_dir, frames, clips_no=5)\n",
    "    if array.shape[0]<400:\n",
    "        array = np.concatenate((array, array), axis=0)\n",
    "    \n",
    "    record[name] = array.shape\n",
    "    y_array = np.ones(shape=(array.shape[0], 1))\n",
    "    y_array = y_array*(index)\n",
    "    \n",
    "    if index == 0:\n",
    "        X_val = array.copy()\n",
    "        y_val = y_array.copy()\n",
    "    else:\n",
    "        X_val = np.concatenate((X_test, array), axis=0)\n",
    "        y_val = np.concatenate((y_test, y_array), axis=0)\n",
    "\n",
    "\n",
    "record = {}\n",
    "for index, name in enumerate(echo_cato):\n",
    "    \n",
    "    print(name)\n",
    "    txt_dir = dir_ + \"test\" + name + \"_name.txt\"\n",
    "    data_dir = dir_ + \"test\" + name + \"_data.txt\"\n",
    "    frames = frames\n",
    "    # clips_no=5\n",
    "    array = generate_npy(txt_dir, data_dir, frames, clips_no=5)\n",
    "    if array.shape[0]<400:\n",
    "        array = np.concatenate((array, array), axis=0)\n",
    "    \n",
    "    record[name] = array.shape\n",
    "    y_array = np.ones(shape=(array.shape[0], 1))\n",
    "    y_array = y_array*(index)\n",
    "    \n",
    "    if index == 0:\n",
    "        X_test = array.copy()\n",
    "        y_test = y_array.copy()\n",
    "    else:\n",
    "        X_test = np.concatenate((X_test, array), axis=0)\n",
    "        y_test = np.concatenate((y_test, y_array), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. plot one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sample = sns.clustermap(X_train[number,::,::].T, \n",
    "                   col_cluster=False, \n",
    "                   # row_cluster=False, \n",
    "                   z_score=0, \n",
    "                   robust = True, \n",
    "                   cmap=\"coolwarm\",\n",
    "                   xticklabels=False, yticklabels=False,\n",
    "                   # square=True,\n",
    "                   figsize=(10,10)\n",
    "                  )\n",
    "one_sample.savefig('....pdf', format=\"pdf\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. build a network for diagnoisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import (\n",
    "    ReduceLROnPlateau,\n",
    "    CSVLogger,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "\n",
    "BZ = 250\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "RG = 50\n",
    "\n",
    "CLASS = 5\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.reshape((y_train.shape[0],))\n",
    "y_train = y_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "y_val = y_val.reshape((y_test.shape[0],))\n",
    "y_val = y_val.astype('float32')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, CLASS)\n",
    "y_val = keras.utils.to_categorical(y_val, CLASS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(2048, 2, activation='relu',padding = \"same\", input_shape=(X_train.shape[1],2048)))\n",
    "model.add(Conv1D(512, 2, activation='relu', dilation_rate = 2, padding = \"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5,seed = 42))\n",
    "model.add(Dense(CLASS, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(lr=LR, momentum=0.9), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)\n",
    "# Checkpoint\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"model_{}_checkpoint_{}_{}.h5\".format(type_name, \"first\", \"title\"),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# csvlogger\n",
    "csv_logger = CSVLogger(\n",
    "    'csv_logger_{}_{}_{}.csv'.format(type_name, \"first\", \"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          epochs= 50, \n",
    "          validation_data = (X_val, y_val), callbacks=[csv_logger, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. plot ROC curves (code from scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_name = \"model_45X2048.h5\"\n",
    "model = load_model(model_name)\n",
    "\n",
    "y_score = model.predict(X_test)\n",
    "n_classes = y_score.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "lw=2\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "lw=2\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.5f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.5f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.5f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
